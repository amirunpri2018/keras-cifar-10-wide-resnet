{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide-Resnet 28x10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 160)  23040       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 160)  230400      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 160)  2560        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 160)  230400      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 160)  230400      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 160)  0           conv2d_6[0][0]                   \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 160)  230400      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 160)  230400      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 160)  0           conv2d_8[0][0]                   \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 160)  230400      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 160)  230400      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 160)  0           conv2d_10[0][0]                  \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 320)  460800      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 320)  921600      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 320)  51200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_12[0][0]                  \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 320)  921600      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 320)  921600      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 320)  0           conv2d_15[0][0]                  \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 320)  921600      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 320)  921600      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 320)  0           conv2d_17[0][0]                  \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 320)  921600      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 320)  921600      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 320)  0           conv2d_19[0][0]                  \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 640)    1843200     activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 640)    3686400     activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 640)    204800      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_21[0][0]                  \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 640)    3686400     activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 640)    3686400     activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 8, 8, 640)    0           conv2d_24[0][0]                  \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 8, 640)    3686400     activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 640)    3686400     activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 640)    0           conv2d_26[0][0]                  \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 640)    3686400     activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 640)    3686400     activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 8, 640)    0           conv2d_28[0][0]                  \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 640)    0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 640)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           6400        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 36,497,136\n",
      "Trainable params: 36,479,184\n",
      "Non-trainable params: 17,952\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/200\n",
      "1563/1563 [==============================] - 359s 230ms/step - loss: 5.3191 - acc: 0.3033 - val_loss: 2.0824 - val_acc: 0.4722\n",
      "Epoch 2/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 1.7249 - acc: 0.5197 - val_loss: 1.4853 - val_acc: 0.5914\n",
      "Epoch 3/200\n",
      "1563/1563 [==============================] - 349s 224ms/step - loss: 1.3950 - acc: 0.6354 - val_loss: 1.5209 - val_acc: 0.6073\n",
      "Epoch 4/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 1.3274 - acc: 0.6784 - val_loss: 1.4768 - val_acc: 0.6449\n",
      "Epoch 5/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.2985 - acc: 0.6986 - val_loss: 1.5511 - val_acc: 0.6213\n",
      "Epoch 6/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.2909 - acc: 0.7074 - val_loss: 1.4846 - val_acc: 0.6634\n",
      "Epoch 7/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 1.2809 - acc: 0.7206 - val_loss: 1.5107 - val_acc: 0.6597\n",
      "Epoch 8/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 1.2789 - acc: 0.7247 - val_loss: 1.3902 - val_acc: 0.6877\n",
      "Epoch 9/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 1.2802 - acc: 0.7283 - val_loss: 1.3371 - val_acc: 0.7235\n",
      "Epoch 10/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.2862 - acc: 0.7310 - val_loss: 1.4318 - val_acc: 0.6769\n",
      "Epoch 11/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.2820 - acc: 0.7360 - val_loss: 1.4002 - val_acc: 0.6981\n",
      "Epoch 12/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.2888 - acc: 0.7364 - val_loss: 1.3067 - val_acc: 0.7354\n",
      "Epoch 13/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.2916 - acc: 0.7403 - val_loss: 1.3477 - val_acc: 0.7222\n",
      "Epoch 14/200\n",
      "1563/1563 [==============================] - 348s 222ms/step - loss: 1.2842 - acc: 0.7416 - val_loss: 1.6156 - val_acc: 0.6617\n",
      "Epoch 15/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.2889 - acc: 0.7428 - val_loss: 1.6910 - val_acc: 0.6152\n",
      "Epoch 16/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.2945 - acc: 0.7427 - val_loss: 1.3906 - val_acc: 0.7204\n",
      "Epoch 17/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.2913 - acc: 0.7455 - val_loss: 1.5342 - val_acc: 0.6781\n",
      "Epoch 18/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.2971 - acc: 0.7465 - val_loss: 1.3059 - val_acc: 0.7436\n",
      "Epoch 19/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3015 - acc: 0.7455 - val_loss: 1.2594 - val_acc: 0.7662014 - acc: 0.\n",
      "Epoch 20/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3041 - acc: 0.7515 - val_loss: 1.7251 - val_acc: 0.6434\n",
      "Epoch 21/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.2989 - acc: 0.7500 - val_loss: 1.9727 - val_acc: 0.5968\n",
      "Epoch 22/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3020 - acc: 0.7492 - val_loss: 1.7136 - val_acc: 0.6431\n",
      "Epoch 23/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3032 - acc: 0.7528 - val_loss: 1.5692 - val_acc: 0.6858\n",
      "Epoch 24/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3004 - acc: 0.7530 - val_loss: 1.5398 - val_acc: 0.6864\n",
      "Epoch 25/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3056 - acc: 0.7540 - val_loss: 1.5404 - val_acc: 0.6941\n",
      "Epoch 26/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3087 - acc: 0.7535 - val_loss: 1.4138 - val_acc: 0.7318\n",
      "Epoch 27/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3180 - acc: 0.7527 - val_loss: 1.3323 - val_acc: 0.7407\n",
      "Epoch 28/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3205 - acc: 0.7545 - val_loss: 2.2074 - val_acc: 0.5968\n",
      "Epoch 29/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3200 - acc: 0.7543 - val_loss: 1.7297 - val_acc: 0.6472\n",
      "Epoch 30/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3145 - acc: 0.7542 - val_loss: 1.4274 - val_acc: 0.7248\n",
      "Epoch 31/200\n",
      "1563/1563 [==============================] - 348s 222ms/step - loss: 1.3138 - acc: 0.7576 - val_loss: 1.2401 - val_acc: 0.7810\n",
      "Epoch 32/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3172 - acc: 0.7563 - val_loss: 1.3507 - val_acc: 0.7442\n",
      "Epoch 33/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3205 - acc: 0.7575 - val_loss: 1.6987 - val_acc: 0.6572\n",
      "Epoch 34/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3220 - acc: 0.7556 - val_loss: 1.5529 - val_acc: 0.6964\n",
      "Epoch 35/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3208 - acc: 0.7580 - val_loss: 1.4338 - val_acc: 0.7181\n",
      "Epoch 36/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3291 - acc: 0.7588 - val_loss: 1.3374 - val_acc: 0.7576\n",
      "Epoch 37/200\n",
      "1563/1563 [==============================] - 348s 222ms/step - loss: 1.3224 - acc: 0.7599 - val_loss: 1.4543 - val_acc: 0.7258\n",
      "Epoch 38/200\n",
      "1563/1563 [==============================] - 348s 222ms/step - loss: 1.3285 - acc: 0.7606 - val_loss: 1.5496 - val_acc: 0.7087\n",
      "Epoch 39/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3355 - acc: 0.7571 - val_loss: 1.3664 - val_acc: 0.7583\n",
      "Epoch 40/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3275 - acc: 0.7603 - val_loss: 1.2996 - val_acc: 0.7739\n",
      "Epoch 41/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 1.3339 - acc: 0.7609 - val_loss: 1.7508 - val_acc: 0.6521\n",
      "Epoch 42/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 1.3305 - acc: 0.7602 - val_loss: 1.4682 - val_acc: 0.7181\n",
      "Epoch 43/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 1.3294 - acc: 0.7630 - val_loss: 1.6099 - val_acc: 0.6854\n",
      "Epoch 44/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 1.3260 - acc: 0.7638 - val_loss: 1.4767 - val_acc: 0.7259\n",
      "Epoch 45/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 1.3362 - acc: 0.7645 - val_loss: 1.3988 - val_acc: 0.7407\n",
      "Epoch 46/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 1.3323 - acc: 0.7623 - val_loss: 1.6927 - val_acc: 0.6746\n",
      "Epoch 47/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 1.3428 - acc: 0.7637 - val_loss: 1.3893 - val_acc: 0.7495\n",
      "Epoch 48/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 1.3388 - acc: 0.7633 - val_loss: 1.3320 - val_acc: 0.7695\n",
      "Epoch 49/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 1.3453 - acc: 0.7627 - val_loss: 1.5097 - val_acc: 0.7110\n",
      "Epoch 50/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 1.3445 - acc: 0.7616 - val_loss: 1.4247 - val_acc: 0.7381\n",
      "Epoch 51/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 1.3421 - acc: 0.7626 - val_loss: 1.6327 - val_acc: 0.6791\n",
      "Epoch 52/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 1.3415 - acc: 0.7627 - val_loss: 1.6206 - val_acc: 0.6917\n",
      "Epoch 53/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 1.3395 - acc: 0.7644 - val_loss: 1.5159 - val_acc: 0.7174\n",
      "Epoch 54/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 1.3522 - acc: 0.7631 - val_loss: 1.6337 - val_acc: 0.6939\n",
      "Epoch 55/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 1.3488 - acc: 0.7634 - val_loss: 1.3031 - val_acc: 0.7776\n",
      "Epoch 56/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 1.3429 - acc: 0.7662 - val_loss: 1.6875 - val_acc: 0.6878\n",
      "Epoch 57/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 1.3437 - acc: 0.7645 - val_loss: 1.4563 - val_acc: 0.7336\n",
      "Epoch 58/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 1.3461 - acc: 0.7661 - val_loss: 1.5016 - val_acc: 0.7261\n",
      "Epoch 59/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 1.3490 - acc: 0.7650 - val_loss: 1.5187 - val_acc: 0.7154\n",
      "Epoch 60/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 1.3553 - acc: 0.7654 - val_loss: 1.3459 - val_acc: 0.7611\n",
      "Epoch 61/200\n",
      "1563/1563 [==============================] - 352s 225ms/step - loss: 0.9679 - acc: 0.8525 - val_loss: 0.8136 - val_acc: 0.8753\n",
      "Epoch 62/200\n",
      "1563/1563 [==============================] - 352s 225ms/step - loss: 0.7884 - acc: 0.8671 - val_loss: 0.7930 - val_acc: 0.8548\n",
      "Epoch 63/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.7562 - acc: 0.8664 - val_loss: 0.7842 - val_acc: 0.8582\n",
      "Epoch 64/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.7512 - acc: 0.8691 - val_loss: 0.8258 - val_acc: 0.8485\n",
      "Epoch 65/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.7527 - acc: 0.8707 - val_loss: 0.8047 - val_acc: 0.8598\n",
      "Epoch 66/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.7549 - acc: 0.8737 - val_loss: 0.7769 - val_acc: 0.8690\n",
      "Epoch 67/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.7597 - acc: 0.8760 - val_loss: 0.8055 - val_acc: 0.8628\n",
      "Epoch 68/200\n",
      "1563/1563 [==============================] - 352s 225ms/step - loss: 0.7653 - acc: 0.8799 - val_loss: 0.8009 - val_acc: 0.8672\n",
      "Epoch 69/200\n",
      "1563/1563 [==============================] - 352s 225ms/step - loss: 0.7667 - acc: 0.8807 - val_loss: 0.8559 - val_acc: 0.8512\n",
      "Epoch 70/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.7700 - acc: 0.8823 - val_loss: 0.7977 - val_acc: 0.8752\n",
      "Epoch 71/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.7732 - acc: 0.8837 - val_loss: 0.8122 - val_acc: 0.8723\n",
      "Epoch 72/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.7747 - acc: 0.8845 - val_loss: 0.8396 - val_acc: 0.8709\n",
      "Epoch 73/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.7844 - acc: 0.8866 - val_loss: 0.8384 - val_acc: 0.8694\n",
      "Epoch 74/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.7863 - acc: 0.8858 - val_loss: 0.8405 - val_acc: 0.8714\n",
      "Epoch 75/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.7850 - acc: 0.8900 - val_loss: 0.8466 - val_acc: 0.8704\n",
      "Epoch 76/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.7955 - acc: 0.8877 - val_loss: 0.8337 - val_acc: 0.8779\n",
      "Epoch 77/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.7995 - acc: 0.8889 - val_loss: 0.8246 - val_acc: 0.8811\n",
      "Epoch 78/200\n",
      "1563/1563 [==============================] - 352s 225ms/step - loss: 0.8035 - acc: 0.8896 - val_loss: 0.8467 - val_acc: 0.8769\n",
      "Epoch 79/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.8082 - acc: 0.8894 - val_loss: 1.0236 - val_acc: 0.8332\n",
      "Epoch 80/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8117 - acc: 0.8897 - val_loss: 0.8923 - val_acc: 0.8668\n",
      "Epoch 81/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8110 - acc: 0.8912 - val_loss: 0.8519 - val_acc: 0.8804\n",
      "Epoch 82/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8152 - acc: 0.8911 - val_loss: 0.8538 - val_acc: 0.8803\n",
      "Epoch 83/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.8111 - acc: 0.8932 - val_loss: 0.8903 - val_acc: 0.8666\n",
      "Epoch 84/200\n",
      "1563/1563 [==============================] - 352s 225ms/step - loss: 0.8198 - acc: 0.8922 - val_loss: 0.8750 - val_acc: 0.8770\n",
      "Epoch 85/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.8146 - acc: 0.8951 - val_loss: 0.8717 - val_acc: 0.8772\n",
      "Epoch 86/200\n",
      "1563/1563 [==============================] - 352s 225ms/step - loss: 0.8278 - acc: 0.8920 - val_loss: 0.9664 - val_acc: 0.8529\n",
      "Epoch 87/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.8290 - acc: 0.8943 - val_loss: 0.9391 - val_acc: 0.8607\n",
      "Epoch 88/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.8260 - acc: 0.8947 - val_loss: 1.0056 - val_acc: 0.8349\n",
      "Epoch 89/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8254 - acc: 0.8937 - val_loss: 0.8355 - val_acc: 0.8927\n",
      "Epoch 90/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.8324 - acc: 0.8948 - val_loss: 0.9025 - val_acc: 0.8771\n",
      "Epoch 91/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.8370 - acc: 0.8961 - val_loss: 0.8723 - val_acc: 0.8852\n",
      "Epoch 92/200\n",
      "1563/1563 [==============================] - 352s 225ms/step - loss: 0.8370 - acc: 0.8939 - val_loss: 0.8808 - val_acc: 0.8823\n",
      "Epoch 93/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 0.8384 - acc: 0.8961 - val_loss: 0.8796 - val_acc: 0.8842\n",
      "Epoch 94/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8420 - acc: 0.8960 - val_loss: 1.0026 - val_acc: 0.8462\n",
      "Epoch 95/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8464 - acc: 0.8952 - val_loss: 0.9239 - val_acc: 0.8741\n",
      "Epoch 96/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8522 - acc: 0.8955 - val_loss: 0.8741 - val_acc: 0.8886\n",
      "Epoch 97/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.8479 - acc: 0.8974 - val_loss: 0.9305 - val_acc: 0.8715\n",
      "Epoch 98/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8509 - acc: 0.8946 - val_loss: 0.9175 - val_acc: 0.8747\n",
      "Epoch 99/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.8568 - acc: 0.8940 - val_loss: 0.8912 - val_acc: 0.8865\n",
      "Epoch 100/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8536 - acc: 0.8972 - val_loss: 0.9118 - val_acc: 0.8804\n",
      "Epoch 101/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8556 - acc: 0.8964 - val_loss: 0.8970 - val_acc: 0.8848\n",
      "Epoch 102/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8574 - acc: 0.8958 - val_loss: 0.9119 - val_acc: 0.8790\n",
      "Epoch 103/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8585 - acc: 0.8956 - val_loss: 0.9142 - val_acc: 0.8774\n",
      "Epoch 104/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8590 - acc: 0.8985 - val_loss: 0.9248 - val_acc: 0.8793\n",
      "Epoch 105/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8612 - acc: 0.8954 - val_loss: 0.8987 - val_acc: 0.8898\n",
      "Epoch 106/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8621 - acc: 0.8969 - val_loss: 0.9111 - val_acc: 0.8841\n",
      "Epoch 107/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8616 - acc: 0.8971 - val_loss: 0.9476 - val_acc: 0.8696\n",
      "Epoch 108/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8624 - acc: 0.8987 - val_loss: 0.9447 - val_acc: 0.8763\n",
      "Epoch 109/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8666 - acc: 0.8979 - val_loss: 0.9451 - val_acc: 0.8758\n",
      "Epoch 110/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8783 - acc: 0.8956 - val_loss: 0.9180 - val_acc: 0.8847\n",
      "Epoch 111/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.8689 - acc: 0.8972 - val_loss: 0.9355 - val_acc: 0.8786\n",
      "Epoch 112/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.8743 - acc: 0.8971 - val_loss: 0.9752 - val_acc: 0.8670\n",
      "Epoch 113/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.8657 - acc: 0.9004 - val_loss: 0.9537 - val_acc: 0.8761\n",
      "Epoch 114/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.8710 - acc: 0.8976 - val_loss: 0.9742 - val_acc: 0.8673\n",
      "Epoch 115/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.8750 - acc: 0.8989 - val_loss: 1.0333 - val_acc: 0.8532\n",
      "Epoch 116/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.8694 - acc: 0.8986 - val_loss: 0.9515 - val_acc: 0.8727\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.8741 - acc: 0.8982 - val_loss: 0.9750 - val_acc: 0.8667\n",
      "Epoch 118/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.8733 - acc: 0.8984 - val_loss: 0.9586 - val_acc: 0.8738\n",
      "Epoch 119/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.8726 - acc: 0.8989 - val_loss: 0.9672 - val_acc: 0.8723\n",
      "Epoch 120/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.8775 - acc: 0.9004 - val_loss: 0.9674 - val_acc: 0.8687\n",
      "Epoch 121/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.7104 - acc: 0.9478 - val_loss: 0.7284 - val_acc: 0.9336\n",
      "Epoch 122/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.6069 - acc: 0.9640 - val_loss: 0.6682 - val_acc: 0.9367\n",
      "Epoch 123/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.5417 - acc: 0.9701 - val_loss: 0.6295 - val_acc: 0.9376\n",
      "Epoch 124/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.4901 - acc: 0.9737 - val_loss: 0.5835 - val_acc: 0.9395\n",
      "Epoch 125/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.4525 - acc: 0.9752 - val_loss: 0.5631 - val_acc: 0.9369\n",
      "Epoch 126/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 0.4228 - acc: 0.9763 - val_loss: 0.5520 - val_acc: 0.9351\n",
      "Epoch 127/200\n",
      "1563/1563 [==============================] - 348s 222ms/step - loss: 0.4020 - acc: 0.9762 - val_loss: 0.5406 - val_acc: 0.9339\n",
      "Epoch 128/200\n",
      "1563/1563 [==============================] - 347s 222ms/step - loss: 0.3900 - acc: 0.9742 - val_loss: 0.5308 - val_acc: 0.9297\n",
      "Epoch 129/200\n",
      "1563/1563 [==============================] - 347s 222ms/step - loss: 0.3773 - acc: 0.9744 - val_loss: 0.5155 - val_acc: 0.9283\n",
      "Epoch 130/200\n",
      "1563/1563 [==============================] - 348s 222ms/step - loss: 0.3732 - acc: 0.9729 - val_loss: 0.5350 - val_acc: 0.9263\n",
      "Epoch 131/200\n",
      "1563/1563 [==============================] - 348s 222ms/step - loss: 0.3713 - acc: 0.9725 - val_loss: 0.5053 - val_acc: 0.9341\n",
      "Epoch 132/200\n",
      "1563/1563 [==============================] - 347s 222ms/step - loss: 0.3676 - acc: 0.9721 - val_loss: 0.5103 - val_acc: 0.9306\n",
      "Epoch 133/200\n",
      "1563/1563 [==============================] - 348s 222ms/step - loss: 0.3684 - acc: 0.9705 - val_loss: 0.5272 - val_acc: 0.9253\n",
      "Epoch 134/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 0.3654 - acc: 0.9714 - val_loss: 0.5133 - val_acc: 0.9276\n",
      "Epoch 135/200\n",
      "1563/1563 [==============================] - 348s 222ms/step - loss: 0.3688 - acc: 0.9706 - val_loss: 0.5402 - val_acc: 0.9236\n",
      "Epoch 136/200\n",
      "1563/1563 [==============================] - 348s 222ms/step - loss: 0.3640 - acc: 0.9722 - val_loss: 0.5231 - val_acc: 0.9285\n",
      "Epoch 137/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 0.3691 - acc: 0.9710 - val_loss: 0.5288 - val_acc: 0.9240\n",
      "Epoch 138/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.3727 - acc: 0.9704 - val_loss: 0.5106 - val_acc: 0.9274\n",
      "Epoch 139/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.3719 - acc: 0.9710 - val_loss: 0.5298 - val_acc: 0.9263\n",
      "Epoch 140/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.3733 - acc: 0.9704 - val_loss: 0.5327 - val_acc: 0.9245\n",
      "Epoch 141/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.3765 - acc: 0.9705 - val_loss: 0.5278 - val_acc: 0.9284\n",
      "Epoch 142/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.3745 - acc: 0.9718 - val_loss: 0.5309 - val_acc: 0.9291\n",
      "Epoch 143/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.3729 - acc: 0.9726 - val_loss: 0.5371 - val_acc: 0.9295\n",
      "Epoch 144/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.3822 - acc: 0.9700 - val_loss: 0.5143 - val_acc: 0.9327\n",
      "Epoch 145/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.3752 - acc: 0.9734 - val_loss: 0.5346 - val_acc: 0.9276\n",
      "Epoch 146/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.3798 - acc: 0.9711 - val_loss: 0.5325 - val_acc: 0.9288\n",
      "Epoch 147/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 0.3770 - acc: 0.9736 - val_loss: 0.5502 - val_acc: 0.9220\n",
      "Epoch 148/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.3816 - acc: 0.9721 - val_loss: 0.5513 - val_acc: 0.9227\n",
      "Epoch 149/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 0.3783 - acc: 0.9737 - val_loss: 0.5879 - val_acc: 0.9193\n",
      "Epoch 150/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.3777 - acc: 0.9738 - val_loss: 0.5423 - val_acc: 0.9261\n",
      "Epoch 151/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.3801 - acc: 0.9734 - val_loss: 0.5748 - val_acc: 0.9209\n",
      "Epoch 152/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.3761 - acc: 0.9750 - val_loss: 0.5545 - val_acc: 0.9273\n",
      "Epoch 153/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.3788 - acc: 0.9737 - val_loss: 0.5836 - val_acc: 0.9174\n",
      "Epoch 154/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.3820 - acc: 0.9728 - val_loss: 0.5411 - val_acc: 0.9266\n",
      "Epoch 155/200\n",
      "1563/1563 [==============================] - 349s 224ms/step - loss: 0.3873 - acc: 0.9718 - val_loss: 0.5582 - val_acc: 0.9250\n",
      "Epoch 156/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.3844 - acc: 0.9730 - val_loss: 0.5613 - val_acc: 0.9221\n",
      "Epoch 157/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.3863 - acc: 0.9731 - val_loss: 0.5729 - val_acc: 0.9216\n",
      "Epoch 158/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.3824 - acc: 0.9745 - val_loss: 0.5891 - val_acc: 0.9158\n",
      "Epoch 159/200\n",
      "1563/1563 [==============================] - 349s 224ms/step - loss: 0.3826 - acc: 0.9751 - val_loss: 0.5544 - val_acc: 0.9255\n",
      "Epoch 160/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 0.3907 - acc: 0.9720 - val_loss: 0.5668 - val_acc: 0.9221\n",
      "Epoch 161/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.3460 - acc: 0.9879 - val_loss: 0.4893 - val_acc: 0.9462\n",
      "Epoch 162/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.3200 - acc: 0.9950 - val_loss: 0.4783 - val_acc: 0.9472\n",
      "Epoch 163/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.3089 - acc: 0.9959 - val_loss: 0.4705 - val_acc: 0.9500\n",
      "Epoch 164/200\n",
      "1563/1563 [==============================] - 349s 224ms/step - loss: 0.3002 - acc: 0.9966 - val_loss: 0.4587 - val_acc: 0.9511\n",
      "Epoch 165/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 0.2909 - acc: 0.9976 - val_loss: 0.4594 - val_acc: 0.9501\n",
      "Epoch 166/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.2836 - acc: 0.9979 - val_loss: 0.4541 - val_acc: 0.9509\n",
      "Epoch 167/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.2756 - acc: 0.9982 - val_loss: 0.4467 - val_acc: 0.9518\n",
      "Epoch 168/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 0.2687 - acc: 0.9985 - val_loss: 0.4445 - val_acc: 0.9505\n",
      "Epoch 169/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.2622 - acc: 0.9986 - val_loss: 0.4437 - val_acc: 0.9511\n",
      "Epoch 170/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.2561 - acc: 0.9986 - val_loss: 0.4352 - val_acc: 0.9525\n",
      "Epoch 171/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 0.2491 - acc: 0.9989 - val_loss: 0.4226 - val_acc: 0.9531\n",
      "Epoch 172/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.2441 - acc: 0.9988 - val_loss: 0.4199 - val_acc: 0.9518\n",
      "Epoch 173/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.2373 - acc: 0.9990 - val_loss: 0.4229 - val_acc: 0.9517\n",
      "Epoch 174/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.2320 - acc: 0.9990 - val_loss: 0.4173 - val_acc: 0.9503\n",
      "Epoch 175/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.2262 - acc: 0.9991 - val_loss: 0.4099 - val_acc: 0.9508\n",
      "Epoch 176/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.2206 - acc: 0.9992 - val_loss: 0.4057 - val_acc: 0.9514\n",
      "Epoch 177/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.2157 - acc: 0.9992 - val_loss: 0.3984 - val_acc: 0.9540\n",
      "Epoch 178/200\n",
      "1563/1563 [==============================] - 352s 225ms/step - loss: 0.2109 - acc: 0.9990 - val_loss: 0.4012 - val_acc: 0.9512\n",
      "Epoch 179/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.2067 - acc: 0.9989 - val_loss: 0.3993 - val_acc: 0.9507\n",
      "Epoch 180/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.2012 - acc: 0.9992 - val_loss: 0.3939 - val_acc: 0.9516\n",
      "Epoch 181/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.1968 - acc: 0.9991 - val_loss: 0.3827 - val_acc: 0.9528\n",
      "Epoch 182/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.1919 - acc: 0.9993 - val_loss: 0.3841 - val_acc: 0.9522\n",
      "Epoch 183/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.1870 - acc: 0.9995 - val_loss: 0.3787 - val_acc: 0.9531\n",
      "Epoch 184/200\n",
      "1563/1563 [==============================] - 351s 225ms/step - loss: 0.1836 - acc: 0.9991 - val_loss: 0.3726 - val_acc: 0.9525\n",
      "Epoch 185/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.1790 - acc: 0.9994 - val_loss: 0.3700 - val_acc: 0.9513\n",
      "Epoch 186/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.1748 - acc: 0.9993 - val_loss: 0.3693 - val_acc: 0.9523\n",
      "Epoch 187/200\n",
      "1563/1563 [==============================] - 350s 224ms/step - loss: 0.1711 - acc: 0.9994 - val_loss: 0.3528 - val_acc: 0.9558\n",
      "Epoch 188/200\n",
      "1563/1563 [==============================] - 349s 224ms/step - loss: 0.1672 - acc: 0.9993 - val_loss: 0.3611 - val_acc: 0.9525\n",
      "Epoch 189/200\n",
      "1563/1563 [==============================] - 351s 224ms/step - loss: 0.1633 - acc: 0.9994 - val_loss: 0.3577 - val_acc: 0.9541\n",
      "Epoch 190/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 0.1604 - acc: 0.9990 - val_loss: 0.3613 - val_acc: 0.9507\n",
      "Epoch 191/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 0.1574 - acc: 0.9990 - val_loss: 0.3481 - val_acc: 0.9514\n",
      "Epoch 192/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 0.1540 - acc: 0.9990 - val_loss: 0.3564 - val_acc: 0.9505\n",
      "Epoch 193/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 0.1510 - acc: 0.9991 - val_loss: 0.3496 - val_acc: 0.9522\n",
      "Epoch 194/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 0.1475 - acc: 0.9991 - val_loss: 0.3501 - val_acc: 0.9509\n",
      "Epoch 195/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 0.1444 - acc: 0.9992 - val_loss: 0.3396 - val_acc: 0.9513\n",
      "Epoch 196/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 0.1426 - acc: 0.9986 - val_loss: 0.3480 - val_acc: 0.9472\n",
      "Epoch 197/200\n",
      "1563/1563 [==============================] - 349s 223ms/step - loss: 0.1398 - acc: 0.9987 - val_loss: 0.3452 - val_acc: 0.9495\n",
      "Epoch 198/200\n",
      "1563/1563 [==============================] - 348s 222ms/step - loss: 0.1375 - acc: 0.9987 - val_loss: 0.3425 - val_acc: 0.9522\n",
      "Epoch 199/200\n",
      "1563/1563 [==============================] - 348s 223ms/step - loss: 0.1369 - acc: 0.9981 - val_loss: 0.3464 - val_acc: 0.9462\n",
      "Epoch 200/200\n",
      "1563/1563 [==============================] - 349s 224ms/step - loss: 0.1354 - acc: 0.9980 - val_loss: 0.3428 - val_acc: 0.9485\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Conv2D, Dense, Input, add, Activation, Flatten, AveragePooling2D\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "DEPTH              = 28\n",
    "WIDE               = 10\n",
    "IN_FILTERS         = 16\n",
    "\n",
    "CLASS_NUM          = 10\n",
    "IMG_ROWS, IMG_COLS = 32, 32\n",
    "IMG_CHANNELS       = 3\n",
    "\n",
    "BATCH_SIZE         = 32\n",
    "EPOCHS             = 200\n",
    "ITERATIONS         = 50000 // BATCH_SIZE + 1\n",
    "WEIGHT_DECAY       = 0.0005\n",
    "LOG_FILE_PATH      = './keras_cifar_10_wide_resnet/'\n",
    "\n",
    "# set GPU memory \n",
    "if('tensorflow' == K.backend()):\n",
    "    import tensorflow as tf\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 60:\n",
    "        return 0.1\n",
    "    if epoch < 120:\n",
    "        return 0.02\n",
    "    if epoch < 160:\n",
    "        return 0.004\n",
    "    return 0.0008\n",
    "\n",
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [125.3, 123.0, 113.9]\n",
    "    std  = [63.0,  62.1,  66.7]\n",
    "    for i in range(3):\n",
    "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
    "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
    "    return x_train, x_test\n",
    "\n",
    "def wide_residual_network(img_input,classes_num,depth,k):\n",
    "    print('Wide-Resnet %dx%d' %(depth, k))\n",
    "    n_filters  = [16, 16*k, 32*k, 64*k]\n",
    "    n_stack    = (depth - 4) // 6\n",
    "\n",
    "    def conv3x3(x,filters):\n",
    "        return Conv2D(filters=filters, kernel_size=(3,3), strides=(1,1), padding='same',\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "        use_bias=False)(x)\n",
    "\n",
    "    def bn_relu(x):\n",
    "        x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    def residual_block(x,out_filters,increase=False):\n",
    "        global IN_FILTERS\n",
    "        stride = (1,1)\n",
    "        if increase:\n",
    "            stride = (2,2)\n",
    "            \n",
    "        o1 = bn_relu(x)\n",
    "        \n",
    "        conv_1 = Conv2D(out_filters,\n",
    "            kernel_size=(3,3),strides=stride,padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "            use_bias=False)(o1)\n",
    "\n",
    "        o2 = bn_relu(conv_1)\n",
    "        \n",
    "        conv_2 = Conv2D(out_filters, \n",
    "            kernel_size=(3,3), strides=(1,1), padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "            use_bias=False)(o2)\n",
    "        if increase or IN_FILTERS != out_filters:\n",
    "            proj = Conv2D(out_filters,\n",
    "                                kernel_size=(1,1),strides=stride,padding='same',\n",
    "                                kernel_initializer='he_normal',\n",
    "                                kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "                                use_bias=False)(o1)\n",
    "            block = add([conv_2, proj])\n",
    "        else:\n",
    "            block = add([conv_2,x])\n",
    "        return block\n",
    "\n",
    "    def wide_residual_layer(x,out_filters,increase=False):\n",
    "        global IN_FILTERS\n",
    "        x = residual_block(x,out_filters,increase)\n",
    "        IN_FILTERS = out_filters\n",
    "        for _ in range(1,int(n_stack)):\n",
    "            x = residual_block(x,out_filters)\n",
    "        return x\n",
    "\n",
    "    x = conv3x3(img_input,n_filters[0])\n",
    "    x = wide_residual_layer(x,n_filters[1])\n",
    "    x = wide_residual_layer(x,n_filters[2],increase=True)\n",
    "    x = wide_residual_layer(x,n_filters[3],increase=True)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D((8,8))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(classes_num,\n",
    "        activation='softmax',\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "        use_bias=False)(x)\n",
    "    return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # load data\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    y_train = keras.utils.to_categorical(y_train, CLASS_NUM)\n",
    "    y_test = keras.utils.to_categorical(y_test, CLASS_NUM)\n",
    "    \n",
    "    # color preprocessing\n",
    "    x_train, x_test = color_preprocessing(x_train, x_test)\n",
    "\n",
    "    # build network\n",
    "    img_input = Input(shape=(IMG_ROWS,IMG_COLS,IMG_CHANNELS))\n",
    "    output = wide_residual_network(img_input,CLASS_NUM,DEPTH,WIDE)\n",
    "    resnet = Model(img_input, output)\n",
    "    print(resnet.summary())\n",
    "    # set optimizer\n",
    "    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "    resnet.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "    # set callback\n",
    "    tb_cb = TensorBoard(log_dir=LOG_FILE_PATH, histogram_freq=0)\n",
    "    change_lr = LearningRateScheduler(scheduler)\n",
    "    cbks = [change_lr,tb_cb]\n",
    "\n",
    "    # set data augmentation\n",
    "    print('Using real-time data augmentation.')\n",
    "    datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "            width_shift_range=0.125,height_shift_range=0.125,fill_mode='reflect')\n",
    "\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # start training\n",
    "    resnet.fit_generator(datagen.flow(x_train, y_train,batch_size=BATCH_SIZE),\n",
    "                        steps_per_epoch=ITERATIONS,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=cbks,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    resnet.save('wresnet.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
